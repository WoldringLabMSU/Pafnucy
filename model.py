import numpy as np

from math import ceil
import io

import tensorflow as tf

from tfbio.net import *


def make_pafnucy_model(isize=20, in_chnls=19, osize=1,
                       conv_patch=5, pool_patch=2, conv_channels=[64, 128, 256],
                       dense_sizes=[1000, 500, 200],
                       lmbda=0.001, learning_rate=1e-5,
                       seed=123):
    """Create network predicting binding affinity from 3D structure of
    protein-ligand complex.

    Network is composed of block of 3D convolutional layers with max pooling
    and block of fully-connected layers with dropout. Created graph is
    organised with variable scopes as follows:
        input
            structure, affinity
        convolutional
            conv0
                w, b, h
            conv1
                w, b, h
            ...
        fully_connected
            fc0
                w, b, h
            fc1
                w, b, h
            ...
        output
            w, b, prediction
        training
            global_step, mse, L2_cost, cost, optimizer, train
    where `input\structure` and `input\affinity` are placeholders for complex
    structure (shape = (batch_size, isize, isize, isize, in_chnls), you can
    create if with `tfbio.data.make_grid`) and target value (shape =
    (batch_size, osize)), respectively. Training operation (`training/train`)
    minimizes RMSE with L2 penalty using Adam optimizer.

    Parameters
    ----------
    isize: int
        Size of a box with structure
    in_chnls: int, optional
        Number of information channels describing the structure. By default it
        is 19 (default number of features generated by tfbio.data.Featurizer)
    osize: int, optional
        Number of output neurons, default = 1
    conv_patch: int, optional
        Size of convolution patch
    pool_patch: int, optional
            Size of max pooling patch
    conv_channels: array-like, shape = (N,), optional
        Numbers of filters in convolutions.
    dense_sizes: array-like, shape = (M,), optional
            Numbers of neurons in fully-connected layers.
    lmbda: float, optional
        Coefficient for L2 penalty
    learning_rate: float, optional
        Learning rate


    Returns
    -------
    graph: tf.Graph
        Graph with defined network and 4 collections: 'input', 'target',
        'output', and 'kp'
    """

    graph = tf.Graph()

    with graph.as_default():
        np.random.seed(seed)
        tf.set_random_seed(seed)
        with tf.variable_scope('input'):
            x = tf.placeholder(tf.float32,
                               shape=(None, isize, isize, isize, in_chnls),
                               name='structure')
            t = tf.placeholder(tf.float32, shape=(None, osize), name='affinity')

        with tf.variable_scope('convolution'):
            h_convs = convolve3D(x, conv_channels,
                                 conv_patch=conv_patch,
                                 pool_patch=pool_patch)
        hfsize = isize
        for _ in range(len(conv_channels)):
            hfsize = ceil(hfsize / pool_patch)
        hfsize = conv_channels[-1] * hfsize**3

        with tf.variable_scope('fully_connected'):
            h_flat = tf.reshape(h_convs, shape=(-1, hfsize), name='h_flat')

            prob1 = tf.constant(1.0, name='keep_prob_default')
            keep_prob = tf.placeholder_with_default(prob1, shape=(),
                                                    name='keep_prob')

            h_fcl = feedforward(h_flat, dense_sizes, keep_prob=keep_prob)

        with tf.variable_scope('output'):
            w = tf.get_variable('w', shape=(dense_sizes[-1], osize),
                                initializer=tf.truncated_normal_initializer(
                                    stddev=(1 / (dense_sizes[-1]**0.5))))
            b = tf.get_variable('b', shape=(osize,), dtype=tf.float32,
                                initializer=tf.constant_initializer(1))
            y = tf.matmul(h_fcl, w) + b

        with tf.variable_scope('training'):
            global_step = tf.get_variable('global_step', shape=(),
                                          initializer=tf.constant_initializer(0),
                                          trainable=False)

            mse = tf.reduce_mean(tf.pow((y - t), 2), name='mse')

            with tf.variable_scope('L2_cost'):
                # sum over all weights
                all_weights = [
                    graph.get_tensor_by_name('convolution/conv%s/w:0' % i)
                    for i in range(len(conv_channels))
                ] + [
                    graph.get_tensor_by_name('fully_connected/fc%s/w:0' % i)
                    for i in range(len(dense_sizes))
                ] + [w]

                l2 = lmbda * tf.reduce_sum([tf.reduce_sum(tf.pow(wi, 2))
                                            for wi in all_weights])

            cost = tf.add(mse, l2, name='cost')

            optimizer = tf.train.AdamOptimizer(learning_rate, name='optimizer')
            train = optimizer.minimize(cost, global_step=global_step,
                                       name='train')

    graph.add_to_collection('output', y)
    graph.add_to_collection('input', x)
    graph.add_to_collection('target', t)
    graph.add_to_collection('kp', keep_prob)

    return graph



